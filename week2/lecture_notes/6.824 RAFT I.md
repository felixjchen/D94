What: 
- Distributed replication protocol
- Based of logs, leader is elected

Problem:
- single point of failure
- e.g. MR coordinator
-  e.g. GFS master
=> avoid split brain syndrome

Example:
- test and set server
- one operation, tes(new_state) -> old_state
- two replicas, two clients, network partitions can desynch two servers
- difficult to tell if flaiure is from server failure or network failure (matters because state and if transacation has actually taken place)
- how do we handle network partitions?

Network Partitions:
- majority rule
- a client will know its operation succeeded if majority of servers confirm
- to tolerate f faults, we need 2f+1
- majority consists of all servers in the system

Quorum Protocols
	- early 1990s: Paxos and View stamped replication
	- 2014: RAFT
	
Replicated State Machine with RAFT
- clients make request to leader server
- server request goes to RAFT lib
- RAFT lib creates a log entry
- server broacasts to all other RAFT libs, replicass duplicate logs
- once logs are replicated (enough)... then leader can guarentee state and reply

- On failure: new leader is elected and takes over role


- Problem : duplicate operations in logs ? if 1st leader dies after committing... Lab3

- We can use many RAFT groups... and shard among them. This will effectively allow us to scale.. Lab4

- How does clients know which is next leader ? list of servers... servers know who leader is

Overview
- Leaders and Followers
- story:
	- client requests to leader
	- leader appends to log
	- leader propogates logs to all followers
	- followers ack propogation
	- leader COMMITS on majority acks. 
	- then the leader's server can safely respond
		- note: followers dont know about commit or majority
	- next operation does two things:	
		1. operation
		2. implies that precending operations have been commited
	- at this point followers know the first operation was commited

Why Logs ? 
- for communication, logs must be able to be resent
- ordered
- persistence on disk, on reboot we can read disk logs
- space for tentative, not sure if commited or not, but can store in log
- ultimately, we strive to achieve consistent logs across all servers

A Log Entry
- logs are indexed
- consists of :
	- command
	- term (leaders term) (termID implicitly signals who the leader is that appended this log entry)
	- log index
- log index + term is unique for each log entry
- Therefore: we want to ensure log consitency of leadership changes, and we need leader election

Leader Election
- leader's job to consitently send append entries (heartbeats)
- followers trigger election when missing heartbeats from leader (election timeout)
	- Election:
		- one vote per term
		- increase term number i+1
		- vote for itself
		- contacts other followers
		- on majority, becomes leader on new term number i+1

- edge cases:
	- leader i was partitioned, comes back, tries to send append entries.... other memebers reject.... leader i steps down.. => no split brain
	- split vote: equal votes on followers, if not careful... this can happen forever, solution, election timeouts are randomized => eventaul success
	- crashes after self elect.... thankfully we saved on disk... otherwise multiply votes per term... cant ever change mind

Election Timeouts
- longer than heartbeat, dont want to election every hearbeat
- practically >= a few heartbeats
- random
- short enough... s.t no long downtimes
- 150ms-250ms


Log Divergence
- worst case... alot of log.. pretty bad (entire log ?)
- cases:
	- failure to proprogate, but self commited, leader has extra entries


10 , 11, 12
3, _, _
3,3,4
3,3,5

- alot of inconsistent cases possible