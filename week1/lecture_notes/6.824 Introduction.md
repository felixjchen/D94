Author: [Frans Kaashoek](http://pdos.csail.mit.edu/~kaashoek/)
URL: https://www.youtube.com/watch?v=WtZ7pcRSkOA&feature=youtu.be
Tags: #literature-note

---

What?
- is composed of many networks, where computers cooperate towards a core task

Why?
- connect physically seperated machines, e.g. sharing files
- scale through parallelism
- fault tollerence
- security via isolation
	
History
- LAN (1980s)
e.g. DNS + email
- Governement allowed commercial business on the web 
- Datacenters and large websites (1990s)
    e.g. web searching, shopping
  - reverse indexing everyhing for web search required alot of compute and data
  - larger user bases also require alot of compute
- Cloud Compute (2000s)
  - compute and data are done inside cloud 
  - everyone has access to ALOT of compute, to build large systems, and can build distributed systems
- people publish papers on distributed system challenges
- active research and development field, boom within last 4 decades


Challenges
1. Many conccurent process accomplishing one goal, hard to justify correctness
2. Fault tolerence, the computation must continue
   - increase complexity 
   - split brain syndrome 
3. quantify performance gains
 
Infrastructure
- Storage
- Computation
- Communication (RPC)
- How can we abstract this to make it useable?
	
Main Topics
- Fault Tolerence
  1. Availability e.g 0.99999
  	Idea: Replication 
  2. Recoverability e.g. how to repair a node
  	Idea: Logging / transacations , disk storage is durable
- Consistency
"strong consistency"
"loss guarentees"
"eventual consistency"
- Performance
- Throughput
- Latency (If one node is slow, what happens to entire system "tail latency")
- In practice, we make trade offs between these three ^
- Implementation
- Building distributed systems has similar challenges, potential failures and many computeres ups complexity
		

